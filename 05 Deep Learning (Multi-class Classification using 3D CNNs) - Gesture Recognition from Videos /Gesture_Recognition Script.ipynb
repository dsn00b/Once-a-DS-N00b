{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "from skimage import transform as skimtr\n",
    "import datetime\n",
    "import os\n",
    "import random as rn\n",
    "from scipy.stats import geom\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed,\\\n",
    "    Flatten, BatchNormalization, Activation, Input, MaxPool3D, Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "rn.seed(30)\n",
    "tf.set_random_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('./Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('./Project_data/val.csv').readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_image_samples = 8\n",
    "prob = 0.5\n",
    "image_index_choice_distribution = 2 # 0: flipped geometric pdf (p=prob); 1: discrete uniform; 2: equal steps sampling (pick up every nth value from distribution)\n",
    "crop_top_pct = 0\n",
    "crop_right_pct = 0\n",
    "crop_bottom_pct = 0\n",
    "crop_left_pct = 0\n",
    "ideal_size_img = (100, 100)\n",
    "normalisation_type = 1 # 0: min-max scaling, 1: min-max scaling using 5th/95th pctl\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function 1: Generate indices for sampling images within a sequence (video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_index(image_index_choice_distribution, num_image_samples, prob):\n",
    "    \n",
    "    if num_image_samples == 30:\n",
    "        \n",
    "        return np.arange(0, 30)\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        if image_index_choice_distribution == 0: # flipped geometric pdf (p=prob)\n",
    "\n",
    "            prob_dist = np.flip(geom.pmf(np.arange(0, 30), prob), axis = 0)\n",
    "            balance = 1 - np.sum(prob_dist)\n",
    "            prob_dist = prob_dist + balance / 30 # to force sum of probabilities to 1\n",
    "\n",
    "            if np.sum(prob_dist) < 1:\n",
    "\n",
    "                prob_dist[-1] = prob_dist[-1] + 1 - np.sum(prob_dist)\n",
    "\n",
    "            elif np.sum(prob_dist) > 1:\n",
    "\n",
    "                prob_dist[0] = prob_dist[0] + 1 - np.sum(prob_dist)\n",
    "\n",
    "            return np.sort(np.random.choice(range(30), size=num_image_samples, replace=False, p = prob_dist))\n",
    "\n",
    "        elif image_index_choice_distribution == 1: # discrete uniform distribution\n",
    "\n",
    "            return np.sort(np.random.choice(range(30), size=num_image_samples, replace=False))\n",
    "\n",
    "        elif image_index_choice_distribution == 2: # equal steps sampling\n",
    "            \n",
    "            # first pass\n",
    "            \n",
    "            image_index = np.flip(list(np.flip(np.arange(0, 30), axis = 0))[::int(np.ceil(30/num_image_samples))])\n",
    "            \n",
    "            # pad until there are num_image_samples elements in image_index\n",
    "            \n",
    "            if len(image_index) != num_image_samples:\n",
    "            \n",
    "                for idx in np.flip(np.arange(0, 30), axis=0):\n",
    "\n",
    "                    if idx not in image_index:\n",
    "\n",
    "                        image_index = np.sort(np.append(image_index, idx))\n",
    "\n",
    "                        if len(image_index) == num_image_samples:\n",
    "\n",
    "                            break\n",
    "            \n",
    "            return image_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function 2: Normalise (Standardise) Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalised_image(image, normalisation_type):\n",
    "    \n",
    "    image = image.astype('float')\n",
    "    \n",
    "    if normalisation_type == 0: # min-max scaling\n",
    "\n",
    "        image = image / 255\n",
    "\n",
    "    elif normalisation_type == 1: # Z-standardisation\n",
    "\n",
    "            pctl_5 = np.percentile(image, 5)\n",
    "            pctl_95 = np.percentile(image, 95)\n",
    "            image = (image - pctl_5) / (pctl_95 - pctl_5)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 100\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = './Project_data/train'\n",
    "val_path = './Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size, ablation = False):\n",
    "    \n",
    "    #print('begin') # debug\n",
    "    print('Source path = ' + source_path)\n",
    "    print('batch size = ', batch_size)\n",
    "    \n",
    "    image_index = generate_image_index(image_index_choice_distribution, num_image_samples, prob)  # sample of images to be used for training\n",
    "    \n",
    "    #print(image_index)\n",
    "    \n",
    "    #i = 0 # debug\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        #print('Epoch Start') # debug\n",
    "        \n",
    "        if ablation == 0:\n",
    "        \n",
    "            randomised_folder_list = np.random.permutation(folder_list)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            randomised_folder_list = (np.random.permutation(folder_list))[0:ablation]\n",
    "        \n",
    "        num_full_size_batches = len(randomised_folder_list) // batch_size\n",
    "        \n",
    "        #print('num training samples: ', len(randomised_folder_list)) # debug\n",
    "        #print('num_full_size_batches:' + str(num_full_size_batches)) # debug\n",
    "        #print('total num image: ' + str(len(randomised_folder_list)*len(image_index))) # debug\n",
    "        #print('len rand folder list: ' + str(len(randomised_folder_list))) # debug\n",
    "        #print('len image index: ' + str(len(image_index))) # debug        \n",
    "        \n",
    "        for batch in range(num_full_size_batches): # loop to generate dataset for all full-batches\n",
    "             \n",
    "            batch_data = np.zeros((batch_size, len(image_index), ideal_size_img[0], ideal_size_img[1], 3)) # initialise batch\n",
    "            batch_labels = np.zeros((batch_size,5)) # initialise one hot representation of labels\n",
    "            \n",
    "            for folder in range(batch_size): # loop to generate dataset for a single full-batch\n",
    "                \n",
    "                # read in file names\n",
    "                image_file_names = os.listdir(source_path+'/'+\\\n",
    "                                    randomised_folder_list[folder + (batch*batch_size)].split(';')[0])\n",
    "                \n",
    "                for idx, item in enumerate(image_index): # loop to read in each image in one batch\n",
    "                    \n",
    "                    # read in image within sequence (video)\n",
    "                    \n",
    "                    image = np.asarray(imageio.imread(source_path+'/'+ randomised_folder_list[folder +\\\n",
    "                            (batch*batch_size)].strip().split(';')[0]+'/'+image_file_names[item]).astype(np.float32))\n",
    "                    \n",
    "                    # crop using hyperparameters:\"crop_top_pct\", \"crop_right_pct\", \"crop_bottom_pct\", \"crop_left_pct\"\n",
    "                    \n",
    "                    crop_start_top = int(np.floor(image.shape[0] * crop_top_pct))\n",
    "                    crop_end_right = int(image.shape[1] - np.ceil(image.shape[1] * crop_right_pct))\n",
    "                    crop_end_bottom = int(image.shape[0] - np.ceil(image.shape[0] * crop_bottom_pct))\n",
    "                    crop_start_left = int(np.floor(image.shape[1] * crop_left_pct))                    \n",
    "                    \n",
    "                    image = image[crop_start_top:crop_end_bottom, crop_start_left:crop_end_right, :]\n",
    "                    \n",
    "                    # resize image\n",
    "                    \n",
    "                    image = skimtr.resize(image, (ideal_size_img[0], ideal_size_img[1]))\n",
    "                    \n",
    "                    # normalise image using hyperparameter \"normalisation_type\" and feed into batch\n",
    "                    \n",
    "                    batch_data[folder, idx, :, :, 0] = normalised_image(image[:, :, 0], normalisation_type)\n",
    "                    batch_data[folder, idx, :, :, 1] = normalised_image(image[:, :, 1], normalisation_type)\n",
    "                    batch_data[folder, idx, :, :, 2] = normalised_image(image[:, :, 2], normalisation_type)\n",
    "                    \n",
    "                    #pass # debug\n",
    "                    \n",
    "                batch_labels[folder, int(randomised_folder_list[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                \n",
    "            #i += 1 # debug\n",
    "            \n",
    "            #print(i) # debug\n",
    "            \n",
    "            yield batch_data, batch_labels\n",
    "            \n",
    "            #yield i # debug\n",
    "        \n",
    "        # code to generate dataset covering remaining folders\n",
    "        \n",
    "        num_remaining_input_seq = len(randomised_folder_list) - num_full_size_batches * batch_size\n",
    "        batch_data = np.zeros((num_remaining_input_seq, len(image_index), ideal_size_img[0], ideal_size_img[1], 3)) # initialise batch\n",
    "        batch_labels = np.zeros((num_remaining_input_seq,5)) # initialise one hot representation of labels\n",
    "\n",
    "        #print('num_remaining_input_seq:' + str(num_remaining_input_seq)) # debug\n",
    "\n",
    "        for idy, folder in enumerate(range(num_full_size_batches * batch_size, len(randomised_folder_list))): # loop through remaining folders\n",
    "            \n",
    "            # read in file names\n",
    "            image_file_names = os.listdir(source_path+'/'+ randomised_folder_list[folder].split(';')[0])\n",
    "\n",
    "            for idx, item in enumerate(image_index): # loop to read in each image in one batch\n",
    "\n",
    "                # read in image within sequence (video)\n",
    "\n",
    "                image = np.asarray(imageio.imread(source_path+'/'+ randomised_folder_list[folder] \\\n",
    "                            .strip().split(';')[0]+'/'+image_file_names[item]).astype(np.float32))\n",
    "\n",
    "                # crop using hyperparameters:\"crop_top_pct\", \"crop_right_pct\", \"crop_bottom_pct\", \"crop_left_pct\"\n",
    "\n",
    "                crop_start_top = int(np.floor(image.shape[0] * crop_top_pct))\n",
    "                crop_end_right = int(image.shape[1] - np.ceil(image.shape[1] * crop_right_pct))\n",
    "                crop_end_bottom = int(image.shape[0] - np.ceil(image.shape[0] * crop_bottom_pct))\n",
    "                crop_start_left = int(np.floor(image.shape[1] * crop_left_pct))                    \n",
    "\n",
    "                image = image[crop_start_top:crop_end_bottom, crop_start_left:crop_end_right, :]\n",
    "\n",
    "                # resize image\n",
    "\n",
    "                image = skimtr.resize(image, (ideal_size_img[0], ideal_size_img[1]))\n",
    "\n",
    "                # normalise image using hyperparameter \"normalisation_type\" and feed into batch\n",
    "\n",
    "                batch_data[idy, idx, :, :, 0] = normalised_image(image[:, :, 0], normalisation_type)\n",
    "                batch_data[idy, idx, :, :, 1] = normalised_image(image[:, :, 1], normalisation_type)\n",
    "                batch_data[idy, idx, :, :, 2] = normalised_image(image[:, :, 2], normalisation_type)\n",
    "                \n",
    "                #pass # debug\n",
    "\n",
    "            batch_labels[idy, int(randomised_folder_list[folder].strip().split(';')[2])] = 1\n",
    "            \n",
    "        #i += 1 # debug\n",
    "        \n",
    "        #print(i) # debug\n",
    "        \n",
    "        yield batch_data, batch_labels\n",
    "    \n",
    "        #break # debug\n",
    "    \n",
    "        #yield i # debug\n",
    "        \n",
    "        #print('Epoch End') # debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 8, 100, 100, 3)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 8, 100, 100, 8)    656       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 100, 100, 8)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 100, 100, 8)    32        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 3, 34, 34, 8)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 3, 34, 34, 8)      32        \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 3, 34, 34, 16)     1168      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 34, 34, 16)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3, 34, 34, 16)     64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 1, 12, 12, 16)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1, 12, 12, 16)     64        \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 1, 12, 12, 32)     13856     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1, 12, 12, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 1, 4, 4, 32)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1, 4, 4, 32)       128       \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 1, 4, 4, 64)       55360     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 4, 4, 64)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 1, 4, 4, 64)       256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 2, 2, 64)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 1, 2, 2, 64)       256       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 123,077\n",
      "Trainable params: 122,085\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set Up Model\n",
    "\n",
    "## Clear prev sessions\n",
    "K.clear_session()\n",
    "\n",
    "## input layer\n",
    "input_layer = Input((num_image_samples, ideal_size_img[0], ideal_size_img[1], 3))\n",
    "\n",
    "## kernel initialiser\n",
    "init = initializers.glorot_normal(seed=None)\n",
    "\n",
    "## conv layers + max pool + batch normalization + droput\n",
    "conv_layer1 = Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', padding='same', kernel_initializer = init)(input_layer)\n",
    "conv_layer1 = Dropout(0.2)(conv_layer1)\n",
    "conv_layer1 = BatchNormalization()(conv_layer1)\n",
    "pooling_layer1 = MaxPool3D(pool_size=(3, 3, 3), padding='same')(conv_layer1)\n",
    "pooling_layer1 = BatchNormalization()(pooling_layer1)\n",
    "\n",
    "## conv layers + max pool + batch normalization + droput\n",
    "conv_layer2 = Conv3D(filters=16, kernel_size=(1, 3, 3), activation='relu', padding='same', kernel_initializer = init)(pooling_layer1)\n",
    "conv_layer2 = Dropout(0.2)(conv_layer2)\n",
    "conv_layer2 = BatchNormalization()(conv_layer2)\n",
    "pooling_layer2 = MaxPool3D(pool_size=(3, 3, 3), padding='same')(conv_layer2)\n",
    "pooling_layer2 = BatchNormalization()(pooling_layer2)\n",
    "\n",
    "## conv layers + max pool + batch normalization + droput\n",
    "conv_layer3 = Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu', padding='same', kernel_initializer = init)(pooling_layer2)\n",
    "conv_layer3 = Dropout(0.2)(conv_layer3)\n",
    "conv_layer3 = BatchNormalization()(conv_layer3)\n",
    "pooling_layer3 = MaxPool3D(pool_size=(1, 3, 3), padding='same')(conv_layer3)\n",
    "pooling_layer3 = BatchNormalization()(pooling_layer3)\n",
    "\n",
    "## conv layers + max pool + batch normalization + droput\n",
    "conv_layer4 = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu', padding='same', kernel_initializer = init)(pooling_layer3)\n",
    "conv_layer4 = Dropout(0.2)(conv_layer4)\n",
    "conv_layer4 = BatchNormalization()(conv_layer4)\n",
    "pooling_layer4 = MaxPool3D(pool_size=(1, 3, 3), padding='same')(conv_layer4)\n",
    "pooling_layer4 = BatchNormalization()(pooling_layer4)\n",
    "\n",
    "\n",
    "## flatten + fully connected layers with dropout\n",
    "flatten_layer = Flatten()(pooling_layer4)\n",
    "dense_layer1 = Dense(units=128, activation='relu', kernel_initializer = init)(flatten_layer)\n",
    "dense_layer1 = Dropout(0.5)(dense_layer1)\n",
    "dense_layer1 = BatchNormalization()(dense_layer1)\n",
    "dense_layer2 = Dense(units=128, activation='relu', kernel_initializer = init)(dense_layer1)\n",
    "dense_layer2 = Dropout(0.5)(dense_layer2)\n",
    "dense_layer2 = BatchNormalization()(dense_layer2)\n",
    "output_layer = Dense(units=5, activation='softmax', kernel_initializer = init)(dense_layer2)\n",
    "\n",
    "## define the model with input layer and output layer\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "## optimizer\n",
    "\n",
    "opt = optimizers.Adam(lr=0.01)\n",
    "\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path = ./Project_data/val\n",
      "batch size =  16\n",
      "Source path = ./Project_data/train\n",
      "batch size =  16\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 45s 1s/step - loss: 1.5675 - categorical_accuracy: 0.3018 - val_loss: 1.4767 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-01-3011_13_07.016235/model-00001-1.56693-0.30015-1.47666-0.42000.h5\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 39s 918ms/step - loss: 1.5319 - categorical_accuracy: 0.2955 - val_loss: 1.4771 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-01-3011_13_07.016235/model-00002-1.53379-0.29563-1.47714-0.37000.h5\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 39s 936ms/step - loss: 1.5313 - categorical_accuracy: 0.3200 - val_loss: 1.4961 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-01-3011_13_07.016235/model-00003-1.52332-0.32428-1.49608-0.30000.h5\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 41s 980ms/step - loss: 1.4724 - categorical_accuracy: 0.3554 - val_loss: 1.5261 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-01-3011_13_07.016235/model-00004-1.47389-0.35445-1.52613-0.28000.h5\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 45s 1s/step - loss: 1.4399 - categorical_accuracy: 0.3606 - val_loss: 1.4761 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-01-3011_13_07.016235/model-00005-1.43752-0.36350-1.47613-0.34000.h5\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 39s 924ms/step - loss: 1.4150 - categorical_accuracy: 0.3990 - val_loss: 1.7056 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-01-3011_13_07.016235/model-00006-1.41555-0.39668-1.70555-0.23000.h5\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 40s 953ms/step - loss: 1.3986 - categorical_accuracy: 0.3852 - val_loss: 1.6069 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-01-3011_13_07.016235/model-00007-1.39578-0.38462-1.60686-0.25000.h5\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1.3381 - categorical_accuracy: 0.4216 - val_loss: 1.4245 - val_categorical_accuracy: 0.3800\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-01-3011_13_07.016235/model-00008-1.33524-0.42534-1.42450-0.38000.h5\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1.2749 - categorical_accuracy: 0.4790 - val_loss: 1.4466 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-01-3011_13_07.016235/model-00009-1.27629-0.47964-1.44665-0.31000.h5\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 46s 1s/step - loss: 1.2854 - categorical_accuracy: 0.4790 - val_loss: 1.5291 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-01-3011_13_07.016235/model-00010-1.28277-0.47964-1.52912-0.24000.h5\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 38s 903ms/step - loss: 1.2589 - categorical_accuracy: 0.4734 - val_loss: 1.3959 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-01-3011_13_07.016235/model-00011-1.25985-0.47210-1.39593-0.39000.h5\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - 39s 928ms/step - loss: 1.1741 - categorical_accuracy: 0.5240 - val_loss: 1.2049 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-01-3011_13_07.016235/model-00012-1.17281-0.52338-1.20487-0.45000.h5\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - 39s 932ms/step - loss: 1.1450 - categorical_accuracy: 0.5240 - val_loss: 1.5196 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-01-3011_13_07.016235/model-00013-1.14665-0.52338-1.51957-0.27000.h5\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - 39s 917ms/step - loss: 1.0808 - categorical_accuracy: 0.5497 - val_loss: 1.2758 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-01-3011_13_07.016235/model-00014-1.07127-0.55505-1.27582-0.32000.h5\n",
      "Epoch 15/100\n",
      "42/42 [==============================] - 38s 915ms/step - loss: 1.0133 - categorical_accuracy: 0.5858 - val_loss: 1.5066 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-01-3011_13_07.016235/model-00015-1.01574-0.58220-1.50658-0.39000.h5\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - 42s 1s/step - loss: 1.0261 - categorical_accuracy: 0.5742 - val_loss: 1.5753 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-01-3011_13_07.016235/model-00016-1.02489-0.57617-1.57529-0.28000.h5\n",
      "Epoch 17/100\n",
      "42/42 [==============================] - 38s 912ms/step - loss: 0.9113 - categorical_accuracy: 0.6364 - val_loss: 1.2618 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-01-3011_13_07.016235/model-00017-0.91441-0.63348-1.26184-0.42000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "Epoch 18/100\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.8729 - categorical_accuracy: 0.6409 - val_loss: 1.2052 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-01-3011_13_07.016235/model-00018-0.87561-0.63801-1.20519-0.46000.h5\n",
      "Epoch 19/100\n",
      "42/42 [==============================] - 40s 944ms/step - loss: 0.8774 - categorical_accuracy: 0.6431 - val_loss: 1.0394 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-01-3011_13_07.016235/model-00019-0.87765-0.64404-1.03944-0.45000.h5\n",
      "Epoch 20/100\n",
      "42/42 [==============================] - 39s 931ms/step - loss: 0.8100 - categorical_accuracy: 0.6889 - val_loss: 1.4872 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-01-3011_13_07.016235/model-00020-0.80655-0.69231-1.48715-0.37000.h5\n",
      "Epoch 21/100\n",
      "42/42 [==============================] - 39s 917ms/step - loss: 0.7978 - categorical_accuracy: 0.7030 - val_loss: 1.0313 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00021: saving model to model_init_2020-01-3011_13_07.016235/model-00021-0.80059-0.70287-1.03128-0.51000.h5\n",
      "Epoch 22/100\n",
      "42/42 [==============================] - 42s 1s/step - loss: 0.7865 - categorical_accuracy: 0.7037 - val_loss: 1.2628 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00022: saving model to model_init_2020-01-3011_13_07.016235/model-00022-0.77612-0.70739-1.26283-0.41000.h5\n",
      "Epoch 23/100\n",
      "42/42 [==============================] - 38s 909ms/step - loss: 0.7047 - categorical_accuracy: 0.7328 - val_loss: 1.2749 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00023: saving model to model_init_2020-01-3011_13_07.016235/model-00023-0.70621-0.73303-1.27494-0.35000.h5\n",
      "Epoch 24/100\n",
      "42/42 [==============================] - 44s 1s/step - loss: 0.7048 - categorical_accuracy: 0.7547 - val_loss: 1.2196 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00024: saving model to model_init_2020-01-3011_13_07.016235/model-00024-0.69882-0.75716-1.21965-0.44000.h5\n",
      "Epoch 25/100\n",
      "42/42 [==============================] - 39s 940ms/step - loss: 0.6845 - categorical_accuracy: 0.7373 - val_loss: 1.1234 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00025: saving model to model_init_2020-01-3011_13_07.016235/model-00025-0.68149-0.73756-1.12342-0.43000.h5\n",
      "Epoch 26/100\n",
      "42/42 [==============================] - 38s 913ms/step - loss: 0.6598 - categorical_accuracy: 0.7361 - val_loss: 1.2121 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00026: saving model to model_init_2020-01-3011_13_07.016235/model-00026-0.65942-0.73454-1.21211-0.40000.h5\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 27/100\n",
      "42/42 [==============================] - 38s 916ms/step - loss: 0.7036 - categorical_accuracy: 0.7071 - val_loss: 1.1541 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00027: saving model to model_init_2020-01-3011_13_07.016235/model-00027-0.70131-0.70890-1.15407-0.45000.h5\n",
      "Epoch 28/100\n",
      "42/42 [==============================] - 39s 923ms/step - loss: 0.6264 - categorical_accuracy: 0.7655 - val_loss: 1.0987 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00028: saving model to model_init_2020-01-3011_13_07.016235/model-00028-0.62433-0.76621-1.09868-0.47000.h5\n",
      "Epoch 29/100\n",
      "42/42 [==============================] - 39s 917ms/step - loss: 0.6663 - categorical_accuracy: 0.7432 - val_loss: 1.0967 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00029: saving model to model_init_2020-01-3011_13_07.016235/model-00029-0.66701-0.74359-1.09671-0.49000.h5\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 45s 1s/step - loss: 0.6251 - categorical_accuracy: 0.7551 - val_loss: 1.0707 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00030: saving model to model_init_2020-01-3011_13_07.016235/model-00030-0.62386-0.75566-1.07065-0.47000.h5\n",
      "Epoch 31/100\n",
      "42/42 [==============================] - 45s 1s/step - loss: 0.6482 - categorical_accuracy: 0.7358 - val_loss: 1.2257 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00031: saving model to model_init_2020-01-3011_13_07.016235/model-00031-0.64523-0.73605-1.22575-0.45000.h5\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 32/100\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.6708 - categorical_accuracy: 0.7473 - val_loss: 1.0721 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00032: saving model to model_init_2020-01-3011_13_07.016235/model-00032-0.66483-0.74962-1.07212-0.51000.h5\n",
      "Epoch 33/100\n",
      "42/42 [==============================] - 40s 955ms/step - loss: 0.6461 - categorical_accuracy: 0.7398 - val_loss: 1.1134 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00033: saving model to model_init_2020-01-3011_13_07.016235/model-00033-0.64550-0.74208-1.11340-0.53000.h5\n",
      "Epoch 34/100\n",
      "42/42 [==============================] - 39s 925ms/step - loss: 0.6251 - categorical_accuracy: 0.7562 - val_loss: 1.2412 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00034: saving model to model_init_2020-01-3011_13_07.016235/model-00034-0.62100-0.75867-1.24117-0.48000.h5\n",
      "Epoch 35/100\n",
      "42/42 [==============================] - 45s 1s/step - loss: 0.6243 - categorical_accuracy: 0.7793 - val_loss: 2.0505 - val_categorical_accuracy: 0.3800\n",
      "\n",
      "Epoch 00035: saving model to model_init_2020-01-3011_13_07.016235/model-00035-0.62587-0.77828-2.05046-0.38000.h5\n",
      "Epoch 36/100\n",
      "42/42 [==============================] - 41s 968ms/step - loss: 0.5714 - categorical_accuracy: 0.7488 - val_loss: 1.0491 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00036: saving model to model_init_2020-01-3011_13_07.016235/model-00036-0.56989-0.75113-1.04909-0.57000.h5\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 37/100\n",
      "42/42 [==============================] - 45s 1s/step - loss: 0.5819 - categorical_accuracy: 0.7562 - val_loss: 1.1349 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00037: saving model to model_init_2020-01-3011_13_07.016235/model-00037-0.57801-0.75867-1.13486-0.41000.h5\n",
      "Epoch 38/100\n",
      "42/42 [==============================] - 45s 1s/step - loss: 0.5537 - categorical_accuracy: 0.7849 - val_loss: 0.9996 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00038: saving model to model_init_2020-01-3011_13_07.016235/model-00038-0.55501-0.78582-0.99958-0.54000.h5\n",
      "Epoch 39/100\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.5167 - categorical_accuracy: 0.7957 - val_loss: 1.0576 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00039: saving model to model_init_2020-01-3011_13_07.016235/model-00039-0.51862-0.79487-1.05762-0.50000.h5\n",
      "Epoch 40/100\n",
      "42/42 [==============================] - 39s 922ms/step - loss: 0.5176 - categorical_accuracy: 0.7942 - val_loss: 1.1752 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00040: saving model to model_init_2020-01-3011_13_07.016235/model-00040-0.51528-0.79336-1.17518-0.50000.h5\n",
      "Epoch 41/100\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.5302 - categorical_accuracy: 0.8027 - val_loss: 1.1555 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00041: saving model to model_init_2020-01-3011_13_07.016235/model-00041-0.52767-0.80392-1.15549-0.46000.h5\n",
      "Epoch 42/100\n",
      "42/42 [==============================] - 42s 997ms/step - loss: 0.5537 - categorical_accuracy: 0.7804 - val_loss: 1.1839 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00042: saving model to model_init_2020-01-3011_13_07.016235/model-00042-0.54881-0.78130-1.18390-0.44000.h5\n",
      "Epoch 43/100\n",
      "42/42 [==============================] - 41s 968ms/step - loss: 0.4931 - categorical_accuracy: 0.8132 - val_loss: 1.0500 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00043: saving model to model_init_2020-01-3011_13_07.016235/model-00043-0.48825-0.81448-1.04998-0.53000.h5\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 44/100\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.5466 - categorical_accuracy: 0.7760 - val_loss: 1.0526 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00044: saving model to model_init_2020-01-3011_13_07.016235/model-00044-0.54387-0.77677-1.05259-0.54000.h5\n",
      "Epoch 45/100\n",
      "42/42 [==============================] - 42s 995ms/step - loss: 0.5117 - categorical_accuracy: 0.7994 - val_loss: 1.0018 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00045: saving model to model_init_2020-01-3011_13_07.016235/model-00045-0.50924-0.80241-1.00179-0.53000.h5\n",
      "Epoch 46/100\n",
      "42/42 [==============================] - 45s 1s/step - loss: 0.5038 - categorical_accuracy: 0.8121 - val_loss: 1.2139 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00046: saving model to model_init_2020-01-3011_13_07.016235/model-00046-0.50340-0.81146-1.21392-0.49000.h5\n",
      "Epoch 47/100\n",
      "42/42 [==============================] - 38s 916ms/step - loss: 0.4505 - categorical_accuracy: 0.8262 - val_loss: 1.3423 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00047: saving model to model_init_2020-01-3011_13_07.016235/model-00047-0.44342-0.82956-1.34228-0.48000.h5\n",
      "Epoch 48/100\n",
      "42/42 [==============================] - 39s 932ms/step - loss: 0.5449 - categorical_accuracy: 0.7976 - val_loss: 1.1095 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00048: saving model to model_init_2020-01-3011_13_07.016235/model-00048-0.54824-0.79487-1.10950-0.51000.h5\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 49/100\n",
      "42/42 [==============================] - 38s 904ms/step - loss: 0.4664 - categorical_accuracy: 0.8403 - val_loss: 1.1561 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00049: saving model to model_init_2020-01-3011_13_07.016235/model-00049-0.46144-0.84012-1.15608-0.55000.h5\n",
      "Epoch 50/100\n",
      "42/42 [==============================] - 38s 914ms/step - loss: 0.4763 - categorical_accuracy: 0.8258 - val_loss: 1.0214 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00050: saving model to model_init_2020-01-3011_13_07.016235/model-00050-0.48037-0.82353-1.02138-0.58000.h5\n",
      "Epoch 51/100\n",
      "42/42 [==============================] - 39s 924ms/step - loss: 0.3971 - categorical_accuracy: 0.8541 - val_loss: 1.0405 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00051: saving model to model_init_2020-01-3011_13_07.016235/model-00051-0.39950-0.85219-1.04046-0.56000.h5\n",
      "Epoch 52/100\n",
      "42/42 [==============================] - 38s 908ms/step - loss: 0.4311 - categorical_accuracy: 0.8489 - val_loss: 1.1049 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00052: saving model to model_init_2020-01-3011_13_07.016235/model-00052-0.41947-0.85068-1.10491-0.57000.h5\n",
      "Epoch 53/100\n",
      "42/42 [==============================] - 40s 964ms/step - loss: 0.4008 - categorical_accuracy: 0.8586 - val_loss: 0.9600 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00053: saving model to model_init_2020-01-3011_13_07.016235/model-00053-0.40265-0.85671-0.95996-0.61000.h5\n",
      "Epoch 54/100\n",
      "42/42 [==============================] - 38s 911ms/step - loss: 0.4437 - categorical_accuracy: 0.8389 - val_loss: 0.9338 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00054: saving model to model_init_2020-01-3011_13_07.016235/model-00054-0.44148-0.83861-0.93380-0.64000.h5\n",
      "Epoch 55/100\n",
      "42/42 [==============================] - 38s 909ms/step - loss: 0.4070 - categorical_accuracy: 0.8366 - val_loss: 1.3565 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00055: saving model to model_init_2020-01-3011_13_07.016235/model-00055-0.40575-0.84012-1.35651-0.50000.h5\n",
      "Epoch 56/100\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.4146 - categorical_accuracy: 0.8463 - val_loss: 1.4853 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00056: saving model to model_init_2020-01-3011_13_07.016235/model-00056-0.41536-0.84615-1.48528-0.44000.h5\n",
      "Epoch 57/100\n",
      "42/42 [==============================] - 40s 952ms/step - loss: 0.3549 - categorical_accuracy: 0.8627 - val_loss: 0.9928 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00057: saving model to model_init_2020-01-3011_13_07.016235/model-00057-0.35462-0.86275-0.99278-0.57000.h5\n",
      "Epoch 58/100\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.3370 - categorical_accuracy: 0.8690 - val_loss: 0.9084 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00058: saving model to model_init_2020-01-3011_13_07.016235/model-00058-0.33996-0.86727-0.90842-0.61000.h5\n",
      "Epoch 59/100\n",
      "42/42 [==============================] - 40s 942ms/step - loss: 0.4812 - categorical_accuracy: 0.8299 - val_loss: 1.0554 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00059: saving model to model_init_2020-01-3011_13_07.016235/model-00059-0.47795-0.82956-1.05544-0.61000.h5\n",
      "Epoch 60/100\n",
      "42/42 [==============================] - 42s 991ms/step - loss: 0.3804 - categorical_accuracy: 0.8534 - val_loss: 1.4008 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00060: saving model to model_init_2020-01-3011_13_07.016235/model-00060-0.38058-0.85520-1.40078-0.58000.h5\n",
      "Epoch 61/100\n",
      "42/42 [==============================] - 38s 909ms/step - loss: 0.4171 - categorical_accuracy: 0.8455 - val_loss: 1.1941 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00061: saving model to model_init_2020-01-3011_13_07.016235/model-00061-0.41330-0.84917-1.19407-0.61000.h5\n",
      "Epoch 62/100\n",
      "42/42 [==============================] - 40s 951ms/step - loss: 0.3199 - categorical_accuracy: 0.8846 - val_loss: 1.2139 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00062: saving model to model_init_2020-01-3011_13_07.016235/model-00062-0.31178-0.88688-1.21387-0.61000.h5\n",
      "Epoch 63/100\n",
      "42/42 [==============================] - 45s 1s/step - loss: 0.3498 - categorical_accuracy: 0.8716 - val_loss: 1.3510 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00063: saving model to model_init_2020-01-3011_13_07.016235/model-00063-0.35246-0.87179-1.35101-0.57000.h5\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 64/100\n",
      "42/42 [==============================] - 39s 918ms/step - loss: 0.3212 - categorical_accuracy: 0.8675 - val_loss: 1.1079 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00064: saving model to model_init_2020-01-3011_13_07.016235/model-00064-0.32273-0.86576-1.10789-0.60000.h5\n",
      "Epoch 65/100\n",
      "42/42 [==============================] - 38s 912ms/step - loss: 0.3450 - categorical_accuracy: 0.8761 - val_loss: 1.6819 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00065: saving model to model_init_2020-01-3011_13_07.016235/model-00065-0.33265-0.87632-1.68189-0.52000.h5\n",
      "Epoch 66/100\n",
      "42/42 [==============================] - 39s 918ms/step - loss: 0.3487 - categorical_accuracy: 0.8753 - val_loss: 1.9201 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00066: saving model to model_init_2020-01-3011_13_07.016235/model-00066-0.34392-0.87934-1.92007-0.49000.h5\n",
      "Epoch 67/100\n",
      "42/42 [==============================] - 38s 902ms/step - loss: 0.3765 - categorical_accuracy: 0.8586 - val_loss: 1.0323 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00067: saving model to model_init_2020-01-3011_13_07.016235/model-00067-0.36189-0.86425-1.03235-0.63000.h5\n",
      "Epoch 68/100\n",
      "42/42 [==============================] - 38s 916ms/step - loss: 0.3773 - categorical_accuracy: 0.8671 - val_loss: 1.0466 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00068: saving model to model_init_2020-01-3011_13_07.016235/model-00068-0.37258-0.86727-1.04660-0.61000.h5\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 69/100\n",
      "42/42 [==============================] - 39s 921ms/step - loss: 0.3953 - categorical_accuracy: 0.8392 - val_loss: 1.0436 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00069: saving model to model_init_2020-01-3011_13_07.016235/model-00069-0.39718-0.83710-1.04360-0.63000.h5\n",
      "Epoch 70/100\n",
      "42/42 [==============================] - 44s 1s/step - loss: 0.3554 - categorical_accuracy: 0.8690 - val_loss: 1.2203 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00070: saving model to model_init_2020-01-3011_13_07.016235/model-00070-0.35885-0.86727-1.22034-0.56000.h5\n",
      "Epoch 71/100\n",
      "42/42 [==============================] - 45s 1s/step - loss: 0.3191 - categorical_accuracy: 0.8980 - val_loss: 0.8845 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00071: saving model to model_init_2020-01-3011_13_07.016235/model-00071-0.31205-0.90045-0.88454-0.61000.h5\n",
      "Epoch 72/100\n",
      "42/42 [==============================] - 45s 1s/step - loss: 0.3395 - categorical_accuracy: 0.8686 - val_loss: 0.8799 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00072: saving model to model_init_2020-01-3011_13_07.016235/model-00072-0.33727-0.86878-0.87988-0.58000.h5\n",
      "Epoch 73/100\n",
      "42/42 [==============================] - 39s 919ms/step - loss: 0.3709 - categorical_accuracy: 0.8545 - val_loss: 0.9488 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00073: saving model to model_init_2020-01-3011_13_07.016235/model-00073-0.36586-0.85822-0.94881-0.63000.h5\n",
      "Epoch 74/100\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.3367 - categorical_accuracy: 0.8854 - val_loss: 0.9922 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00074: saving model to model_init_2020-01-3011_13_07.016235/model-00074-0.33973-0.88386-0.99216-0.60000.h5\n",
      "Epoch 75/100\n",
      "42/42 [==============================] - 38s 912ms/step - loss: 0.2916 - categorical_accuracy: 0.9021 - val_loss: 0.9215 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00075: saving model to model_init_2020-01-3011_13_07.016235/model-00075-0.27641-0.90649-0.92149-0.61000.h5\n",
      "Epoch 76/100\n",
      "42/42 [==============================] - 39s 926ms/step - loss: 0.4496 - categorical_accuracy: 0.8295 - val_loss: 1.0223 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00076: saving model to model_init_2020-01-3011_13_07.016235/model-00076-0.44743-0.83107-1.02227-0.61000.h5\n",
      "Epoch 77/100\n",
      "42/42 [==============================] - 40s 961ms/step - loss: 0.3578 - categorical_accuracy: 0.8697 - val_loss: 1.0600 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00077: saving model to model_init_2020-01-3011_13_07.016235/model-00077-0.35542-0.87179-1.05998-0.55000.h5\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 78/100\n",
      "42/42 [==============================] - 39s 917ms/step - loss: 0.3826 - categorical_accuracy: 0.8574 - val_loss: 1.1321 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00078: saving model to model_init_2020-01-3011_13_07.016235/model-00078-0.37376-0.86124-1.13213-0.53000.h5\n",
      "Epoch 79/100\n",
      "42/42 [==============================] - 39s 920ms/step - loss: 0.4197 - categorical_accuracy: 0.8534 - val_loss: 0.8999 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00079: saving model to model_init_2020-01-3011_13_07.016235/model-00079-0.41449-0.85520-0.89992-0.64000.h5\n",
      "Epoch 80/100\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.3160 - categorical_accuracy: 0.8898 - val_loss: 0.9202 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00080: saving model to model_init_2020-01-3011_13_07.016235/model-00080-0.31881-0.88839-0.92025-0.60000.h5\n",
      "Epoch 81/100\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.3676 - categorical_accuracy: 0.8690 - val_loss: 1.0141 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00081: saving model to model_init_2020-01-3011_13_07.016235/model-00081-0.37132-0.86727-1.01408-0.56000.h5\n",
      "Epoch 82/100\n",
      "42/42 [==============================] - 40s 962ms/step - loss: 0.2930 - categorical_accuracy: 0.8928 - val_loss: 1.0623 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00082: saving model to model_init_2020-01-3011_13_07.016235/model-00082-0.29499-0.89140-1.06227-0.58000.h5\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 83/100\n",
      "42/42 [==============================] - 38s 909ms/step - loss: 0.2996 - categorical_accuracy: 0.8954 - val_loss: 1.1977 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00083: saving model to model_init_2020-01-3011_13_07.016235/model-00083-0.29885-0.89593-1.19769-0.57000.h5\n",
      "Epoch 84/100\n",
      "42/42 [==============================] - 39s 919ms/step - loss: 0.3093 - categorical_accuracy: 0.8865 - val_loss: 1.0643 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00084: saving model to model_init_2020-01-3011_13_07.016235/model-00084-0.30803-0.88688-1.06430-0.57000.h5\n",
      "Epoch 85/100\n",
      "42/42 [==============================] - 44s 1s/step - loss: 0.2931 - categorical_accuracy: 0.9092 - val_loss: 1.2244 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00085: saving model to model_init_2020-01-3011_13_07.016235/model-00085-0.29258-0.90799-1.22437-0.61000.h5\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 41s 985ms/step - loss: 0.2678 - categorical_accuracy: 0.9114 - val_loss: 0.8936 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00086: saving model to model_init_2020-01-3011_13_07.016235/model-00086-0.25573-0.91403-0.89363-0.64000.h5\n",
      "Epoch 87/100\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.2785 - categorical_accuracy: 0.9077 - val_loss: 0.7904 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00087: saving model to model_init_2020-01-3011_13_07.016235/model-00087-0.27940-0.90649-0.79040-0.66000.h5\n",
      "Epoch 88/100\n",
      "42/42 [==============================] - 45s 1s/step - loss: 0.2730 - categorical_accuracy: 0.9133 - val_loss: 1.0938 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00088: saving model to model_init_2020-01-3011_13_07.016235/model-00088-0.27090-0.91403-1.09378-0.61000.h5\n",
      "Epoch 89/100\n",
      "42/42 [==============================] - 39s 937ms/step - loss: 0.2769 - categorical_accuracy: 0.9010 - val_loss: 1.0227 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00089: saving model to model_init_2020-01-3011_13_07.016235/model-00089-0.26416-0.90347-1.02265-0.58000.h5\n",
      "Epoch 90/100\n",
      "42/42 [==============================] - 39s 933ms/step - loss: 0.3197 - categorical_accuracy: 0.8880 - val_loss: 1.0478 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00090: saving model to model_init_2020-01-3011_13_07.016235/model-00090-0.31133-0.88839-1.04782-0.60000.h5\n",
      "Epoch 91/100\n",
      "42/42 [==============================] - 43s 1s/step - loss: 0.2720 - categorical_accuracy: 0.9069 - val_loss: 1.1659 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00091: saving model to model_init_2020-01-3011_13_07.016235/model-00091-0.26311-0.90950-1.16593-0.57000.h5\n",
      "Epoch 92/100\n",
      "42/42 [==============================] - 39s 936ms/step - loss: 0.2622 - categorical_accuracy: 0.9047 - val_loss: 1.1025 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00092: saving model to model_init_2020-01-3011_13_07.016235/model-00092-0.26425-0.90347-1.10245-0.54000.h5\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 93/100\n",
      "42/42 [==============================] - 39s 919ms/step - loss: 0.2591 - categorical_accuracy: 0.9073 - val_loss: 0.9146 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00093: saving model to model_init_2020-01-3011_13_07.016235/model-00093-0.25855-0.90799-0.91460-0.63000.h5\n",
      "Epoch 94/100\n",
      "42/42 [==============================] - 39s 932ms/step - loss: 0.2682 - categorical_accuracy: 0.9092 - val_loss: 0.9529 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00094: saving model to model_init_2020-01-3011_13_07.016235/model-00094-0.26989-0.90799-0.95290-0.63000.h5\n",
      "Epoch 95/100\n",
      "42/42 [==============================] - 38s 910ms/step - loss: 0.2662 - categorical_accuracy: 0.9129 - val_loss: 1.1038 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00095: saving model to model_init_2020-01-3011_13_07.016235/model-00095-0.26080-0.91554-1.10383-0.55000.h5\n",
      "Epoch 96/100\n",
      "42/42 [==============================] - 39s 938ms/step - loss: 0.2124 - categorical_accuracy: 0.9285 - val_loss: 0.8888 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00096: saving model to model_init_2020-01-3011_13_07.016235/model-00096-0.21469-0.92760-0.88878-0.67000.h5\n",
      "Epoch 97/100\n",
      "42/42 [==============================] - 44s 1s/step - loss: 0.2397 - categorical_accuracy: 0.9177 - val_loss: 0.9827 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00097: saving model to model_init_2020-01-3011_13_07.016235/model-00097-0.24097-0.91855-0.98267-0.61000.h5\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 98/100\n",
      "42/42 [==============================] - 44s 1s/step - loss: 0.2277 - categorical_accuracy: 0.9248 - val_loss: 0.8693 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00098: saving model to model_init_2020-01-3011_13_07.016235/model-00098-0.21988-0.92760-0.86935-0.66000.h5\n",
      "Epoch 99/100\n",
      "42/42 [==============================] - 38s 905ms/step - loss: 0.2347 - categorical_accuracy: 0.9222 - val_loss: 0.9500 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00099: saving model to model_init_2020-01-3011_13_07.016235/model-00099-0.23603-0.92308-0.94997-0.63000.h5\n",
      "Epoch 100/100\n",
      "42/42 [==============================] - 41s 978ms/step - loss: 0.1931 - categorical_accuracy: 0.9300 - val_loss: 1.3016 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00100: saving model to model_init_2020-01-3011_13_07.016235/model-00100-0.19420-0.92911-1.30158-0.55000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4ec3edcb70>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Choose num training samples\n",
    "ablation = 0\n",
    "\n",
    "## Set up Batch Size\n",
    "batch_size = 2**4\n",
    "\n",
    "\n",
    "## Compute steps_per_epoch\n",
    "\n",
    "if ablation > 0:\n",
    "    \n",
    "    train_steps_per_epoch = np.ceil(ablation/batch_size)\n",
    "    val_steps_per_epoch = np.ceil(ablation/batch_size)\n",
    "\n",
    "else:\n",
    "    \n",
    "    train_steps_per_epoch = np.ceil(num_train_sequences/batch_size)\n",
    "    val_steps_per_epoch = np.ceil(num_val_sequences/batch_size)\n",
    "\n",
    "## Instantiate generator\n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size, ablation)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "## Build Callback to Reduce LR on Plateau\n",
    "\n",
    "LROP = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.001, verbose=1)\n",
    "\n",
    "## Build Checkpoint Callback to record Validation Dataset Loss\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "\n",
    "## Fit\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=train_steps_per_epoch,\n",
    "                    epochs=num_epochs, verbose=1, callbacks=[checkpoint, LROP], \n",
    "                    validation_data=val_generator, validation_steps=val_steps_per_epoch,\n",
    "                    class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
